# NOTE: This buildkite pipeline is saved in the WebUI

agents:
  queue: "julia"
  # Only run on `sandbox.jl` machines (not `docker`-isolated ones) since we need nestable sandboxing
  sandbox.jl: "true"
  os: "linux"
steps:
  - label: ":rocket: sync logs"
    plugins:
      - staticfloat/cryptic#v1:
          variables:
            - AWS_ACCESS_KEY_ID="U2FsdGVkX1/H9b08vYfGLIy0xYlKBA+pkT5XAjh5XrQdd5XNbjyVFXhv2XRsG13h"
            - AWS_SECRET_ACCESS_KEY="U2FsdGVkX1/YhdG5Jr9ywTjysCrjQIoAlfrVT8qMPzBoPttuG3NIMorn3LLcWpp657MEPhjib+MQ/At1qllwBg=="
          files:
            - ".buildkite/secrets/id_rsa"
      - staticfloat/ssh-agent:
          keyfiles:
            - .buildkite/secrets/id_rsa

      # Install Julia for sandbox
      - JuliaCI/julia#v1:
          version: "1.6"
          persist_depo_dirs: registries,packages,artifacts,compiled,scratchspaces
      - staticfloat/sandbox:
          rootfs_url: "https://github.com/JuliaCI/rootfs-images/releases/download/v3.14/pkgserver_logsync.x86_64.tar.gz"
          rootfs_treehash: "d2566c22a9bb357042e1135cf968e2c7fd32a54a"
          workspaces:
            - "$${BUILDKITE_PLUGIN_JULIA_CACHE_DIR}:$${BUILDKITE_PLUGIN_JULIA_CACHE_DIR}"
          verbose: "true"
    timeout_in_minutes: 120
    # Only allow one job to run at a time, to prevent us clobbering the bucket
    concurrency: 1
    concurrency_group: "JuliaPackaging/PkgServerLogAnalysis.jl/s3_sync_job.yml"

    commands: |
      echo "--- Instantiate project"
      LOG_DIR=$${JULIA_DEPOT_PATH}/scratchspaces/736c6f6f-5473-6973-796c-616e41676f4c/raw_logs
      CSV_DIR=$${JULIA_DEPOT_PATH}/scratchspaces/736c6f6f-5473-6973-796c-616e41676f4c/raw_csvs
      SANITIZED_CSV_DIR=$${JULIA_DEPOT_PATH}/scratchspaces/736c6f6f-5473-6973-796c-616e41676f4c/sanitized_csvs
      mkdir -p "$${LOG_DIR}"

      # Instantiate our project
      julia --project -e 'import Pkg; Pkg.instantiate()'
      
      # Get list of servers
      SERVERS=( $$(julia --project bin/get_server_list.jl) )

      echo "--- Downloading logs from $${#SERVERS[@]} servers"
      for IDX in $${!SERVERS[@]}; do
        SERVER="$$(echo "$${SERVERS[$${IDX}]}" | sed -e 's&^https://&&')"
        echo " -> [$$IDX/$${#SERVERS[@]}] $${SERVER}"

        USER=ubuntu
        if [[ "$${SERVER}" == cn-* ]]; then
          USER=centos
        fi
        # Ignore return code from `rsync` so that we can skip failing servers
        # Also limit each transfer to 120s, so that we don't end up eating huge amounts of
        # CI time if a chinese server is feeling particularly slow today
        (timeout 120s rsync -Prt -e "ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -oBatchMode=yes" \
                      "$${USER}@$${SERVER}:~/src/PkgServer.jl/deployment/logs/nginx/access_*.gz" \
                      "$${LOG_DIR}" || true) &
      done

      # Wait for all those `rsync` commands to finish
      wait

      echo "--- Uploading raw (gzip-compressed) logs to S3 (these expire after 30 days)"
      GZIPPED_RAW_LOGS=( $${LOG_DIR}/*.gz )
      aws s3 sync --acl=private "$${LOG_DIR}" "s3://julialang-pkgserver-logs/raw/"

      echo "--- Checking which of the $${#GZIPPED_RAW_LOGS[@]} logfiles need to be parsed"
      LOGS_TO_PARSE_LOCKFILE=$$(mktemp)
      LOGS_TO_PARSE_FILE=$$(mktemp)
      function check_s3_csv_zst_exists() {
        local key="csv/$$(basename $${1%.*}).csv.zst"
        if ! aws s3api head-object --bucket julialang-pkgserver-logs-sanitized --key $${key} >/dev/null 2>/dev/null; then
          (
            flock -x 200
            echo "$$1" >> "$${LOGS_TO_PARSE_FILE}"
          )  200>$${LOGS_TO_PARSE_LOCKFILE}
        fi
      }

      # Launch parallel jobs to probe existence of .csv.zst files on s3
      for f in $${GZIPPED_RAW_LOGS[@]}; do
        check_s3_csv_zst_exists "$${f}" &
      done
      wait

      # Read in the identified logs to parse
      LOGS_TO_PARSE=( $$(cat $${LOGS_TO_PARSE_FILE}) )
      rm -f $${LOGS_TO_PARSE_FILE} $${LOGS_TO_PARSE_LOCKFILE}

      echo "+++ Parsing $${#LOGS_TO_PARSE[@]} logfiles"
      julia --project bin/parse_logfiles.jl "$${LOGS_TO_PARSE[@]}"

      echo "+++ Sanitizing newly-created CSVs"
      CSVS_TO_SANITIZE=()
      for f in $${LOGS_TO_PARSE[@]}; do
        CSVS_TO_SANITIZE+=( "$${CSV_DIR}/$$(basename $${f%.*}).csv.zst" )
      done
      julia --project bin/sanitize_csvs.jl "$${SANITIZED_CSV_DIR}" "$${CSVS_TO_SANITIZE[@]}"

      echo "+++ Uploading '.csv.zst' files to S3"
      for f in $${LOGS_TO_PARSE[@]}; do
        csv_zst_name="$$(basename $${f%.*}).csv.zst"

        # Upload raw `.csv.zst` file; this gets deleted after 30 days
        aws s3 cp --acl=private "$${CSV_DIR}/$${csv_zst_name}" "s3://julialang-pkgserver-logs/csv/$${csv_zst_name}"

        # Upload sanitized `.csv.zst` file; this stays forever
        aws s3 cp --acl=private "$${SANITIZED_CSV_DIR}/$${csv_zst_name}" "s3://julialang-pkgserver-logs-sanitized/csv/$${csv_zst_name}"
      done
